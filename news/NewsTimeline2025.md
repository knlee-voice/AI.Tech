
### 2025 Academic Conference 
* (12/2-12/7) NeurIPS 2025 - https://neurips.cc/ (@San Diego)
* (8/17-8/21) Interspeech 2025 - https://www.interspeech2025.org/ (@Rotterdam, The Netherlands)
* (7/27~8/1) ACL 2025 - https://2025.aclweb.org/ (@Vienna, Austria)
* (6/10~6/17) CVPR 2025 - https://cvpr.thecvf.com/Conferences/2025
* (4/29-5/4) NAACL 2025 - https://2025.naacl.org/
* (4/24-4/28) ICLR 2025 - https://iclr.cc/Conferences/2025/ 
* (2/25~3/4) AAAI 2025 - https://aaai.org/conference/aaai/aaai-25/ 
* (1/19-1/24) COLING 2025 - https://coling2025.org/
 
### 2025 AI Summit / AI Conference for Developers
* (11/10-11) AI Summit Seoul & EXPO 2025 - https://www.aisummitseoul.com/
* (5/14-15) AWS Summit Seoul 2025 - https://aws.amazon.com/ko/events/summits/seoul/ 
* (3/17-3/21) GTC 2025 - https://www.nvidia.com/ko-kr/gtc/, [세션 카탈로그](https://www.nvidia.com/ko-kr/gtc/session-catalog/)

### Sep. 2025 
* (9/26) 구글, 제미나이 2.5 플래시(gemini-flash-latest’)'와 '제미나이 2.5 플래시 라이트(gemini-flash-lite-latest) 프리뷰' 버전을 공개
* (9/23) 알리바바, 멀티모달 '큐원3-옴니(Qwen3-Omni) 30B' 오픈 소스 출시 - [HF](https://huggingface.co/collections/Qwen/qwen3-omni-68d100a86cd0906843ceccbe), [Qwen3-Omni Demo](https://modelscope.cn/studios/Qwen/Qwen3-Omni-Demo), [AI타임즈](https://www.aitimes.com/news/articleView.html?idxno=202652)
  - ▲ 멀티모달 입출력을 모두 지원하는 인스트럭트(Instruct) 모델 'Qwen3-Omni-30B-A3B-Instruct'
  - ▲ 추론과 장기적 사고에 특화된 싱킹(Thinking) 모델 'Qwen3-Omni-30B-A3B-Thinking'
  - ▲ 오디오 캡셔닝에 최적화된 캡셔너(Captioner) 모델 'Qwen3-Omni-30B-A3B-Captioner'
  - 지원 언어는 텍스트 119개, 음성 입력 19개, 음성 출력 10개로 한국어를 포함한 주요 언어와 광둥어 같은 방언까지 포함
  - 훈련 과정에서는 약 2조 토큰 규모의 데이터와 2000만 시간 분량의 오디오를 활용
* (9/23) MIT, LLM '계획 능력' 강화하는 명령어 프레임워크 ‘PDDL-인스트럭트(Planning Domain Definition Language-Instruct)’ - [arXiv](https://arxiv.org/pdf/2509.13351), [AI타임즈](https://www.aitimes.com/news/articleView.html?idxno=202670)
* (9/16) 구글 AI 학습 데이터 '정제법' 발표 - [arXiv](https://arxiv.org/pdf/2509.08653), [AI타임즈](https://www.aitimes.com/news/articleView.html?idxno=202466)
* (9/13) 알리바바, Qwen3-Next-80B-A3B 오픈소스 출시(Instruct, Thinking - 2가지 버전), "속도 10배·비용 1/10" (하이브리드 설계)- [HF](https://huggingface.co/collections/Qwen/qwen3-next-68c25fd6838e585db8eeea9d), [AI타임즈](https://www.aitimes.com/news/articleView.html?idxno=202391)
* (9/12) 텐센트, 'Parallel-R1': 강화 학습을 통한 병렬 사고 구현 - [arXiv](https://arxiv.org/pdf/2509.07980)
* (9/10) 화웨이, AI 에이전트 '실시간 학습' 기법(메멘토(Memento)) - [arXiv](https://arxiv.org/abs/2508.16153), [git/GibsonAI/memori](https://github.com/GibsonAI/memori), [AI타임즈](https://www.aitimes.com/news/articleView.html?idxno=202259)

### July 2025 
* (7/15) AI 챗봇 앱 사용자 '탑3'는 챗GPT·제타·뤼튼 (한국인 스마트폰 사용자 표본 조사) - [뉴스1](https://news.nate.com/view/20250715n15599), [뉴스2](https://www.sedaily.com/NewsView/2GVDEHXWD2), [뉴스3](https://www.mk.co.kr/news/it/11368367)
  - MAU 1천844만명, 제타(304만명), 뤼튼(245만명), 퍼플렉시티(171만명), 에이닷(138만명) 등

### Jun. 2025 
* (6/29) MIT, 스스로 학습하고 진화하는 언어 모델 프레임워크 ‘SEAL(Self-Adapting Language Models)’ 공개 - [arXiv](https://arxiv.org/pdf/2506.10943), [github](https://github.com/Continual-Intelligence)[AI타임즈](https://www.aitimes.com/news/articleView.html?idxno=200039)
* (6/25) 사카나 AI, 추론 모델 증류 성능 높이는 '강화 학습 교사(RLT)' - [arXiv](https://www.arxiv.org/pdf/2506.08388), [AI타임즈](https://www.aitimes.com/news/articleView.html?idxno=200087)

### May. 2025
* (5/14) 사카나, 트랜스포머 넘는 단계별 사고 방식 'CTM(Continuous Thought Machines)' 아키텍처 공개 - [git](https://github.com/SakanaAI/continuous-thought-machines), [AI타임즈](https://www.aitimes.com/news/articleView.html?idxno=170432)
  -  메모: 뉴런이 ‘틱(tick)’이라는 짧은 시간 단위로 단계별 계산을 진행, 각 뉴런이 스스로 상태(언제, 왜 활성화됐는지에 대한 짧은 기록)를 기억하고, 그에 따라 작동 방식을 조절(다시 작동할지 스스로 결정, 시간을 유연하게 조절) -> 효율적이고 유연함.
* (5/13) 자연어로 원하는 블럭 만들어 주는 '레고GPT' 공개(라마-3.2-1B-인스트럭트 모델 기반, 4만7천개 StableText2Lego 데이터셋) - [AI타임즈](https://www.aitimes.com/news/articleView.html?idxno=170352) 
* (5/6) 엔비디아, 60분짜리 오디오 1초 만에 받아 쓰는 전사 모델 공개(parakeet-tdt-0.6b-v2)' - [HF](https://huggingface.co/nvidia/parakeet-tdt-0.6b-v2), [AI타임즈](https://www.aitimes.com/news/articleView.html?idxno=170202)

### Apr. 2025 
* (4/23) 엔비디아, 긴 영상일수록 분석 능력 향상하는 VLM '이글 2.5' 출시 - [arXiv](https://arxiv.org/pdf/2504.15271), [git](https://github.com/NVlabs/EAGLE), [AI타임즈](https://www.aitimes.com/news/articleView.html?idxno=169887)
  - 학습 전략: '정보 우선 샘플링(Information-First Sampling): 이미지 영역 보존(IAP) 기법, 자동 저하 샘플링(ADS)'과 '점진적 후속 학습(Progressive Post-Training): 학습 단계에서 32K, 64K, 128K 토큰 크기의 입력을 순차적으로 입력' 도입
* (4/15) 오픈AI, 코딩 강화한 개발자용 'GPT-4.1' API 출시 - [AI타임즈](https://www.aitimes.com/news/articleView.html?idxno=169619)
  - 추론 기능은 없음 / 100만 토큰의 컨텍스트 창을 지원, 약 75만 단어를 한번에 처리 / '제미나이 2.5 프로'와 '클로드 3.7 소네트'를 출시하며 코딩 능력을 부각
  - 사용자 피드백 반영: 프런트엔드 코딩, 불필요한 편집 감소, 안정적인 형식 준수, 응답 구조 및 순서 준수, 일관된 도구 사용 등
* (4/14) 자연어 입력하면 실제 로봇 설계해 주는 생성 AI 프레임워크 '텍스트2로봇(Text2Robot)' - [arXiv](https://arxiv.org/pdf/2406.19963), [git](https://github.com/generalroboticslab/Text2Robot), [AI타임즈](https://www.aitimes.com/news/articleView.html?idxno=169606)
   - 1) Text-to-Mesh, 2) Mesh-to-Robot Model, 3) Co-optimizing Morphology and Walking Policy, 4) Physical Assembly 
* (4/6) 오픈 소스 AI 검색 프레임워크 'ODS(Open Deep Search)' 공개 - [arXiv](https://arxiv.org/pdf/2503.20201), [git](https://github.com/sentient-agi/OpenDeepSearch), [AI타임즈](https://www.aitimes.com/news/articleView.html?idxno=169375)
* (4/3) 오픈AI, 'ML 논문 실행'으로 AI 평가하는 벤치마크 'PaperBench' 출시 - [github](https://github.com/openai/preparedness/tree/main/project/paperbench), [AI타임즈](https://www.aitimes.com/news/articleView.html?idxno=169331)
   - ICML 2024에서 선정된 20개 논문을 대상으로, 8316개의 과제로 세분, LLM으로 평가, SimpleJudge 모델 사용
   - 클로드 3.5 소네트'가 평균 21%의 점수로 가장 높은 성능을 기록, 인간 전문가는 41.4%로, AI를 압도

### Mar. 2025 
* (3/23) '트랜스포머+확산' 하이브리드 이미지 생성 도구 ‘HART(Hybrid Autoregressive Transformer)’ - [arXiv](https://arxiv.org/pdf/2410.10812), [AI타임즈](https://www.aitimes.com/news/articleView.html?idxno=169005)
* (3/22) 바이트댄스, 딥시크 RL기법 개선한 RL알고리즘 공개 'DAPO(Decoupled Clip and Dynamic Sampling Policy Optimisation)' - [git](https://github.com/BytedTsinghua-SIA/DAPO), [AI타임즈](https://www.aitimes.com/news/articleView.html?idxno=168989)
* (3/20) RAG 필요 없이 LLM이 '자율 검색'하는 추론 기술 '서치-R1(SEARCH-R1)' - [arXiv](https://arxiv.org/pdf/2503.09516), [git](https://github.com/PeterGriffinJin/Search-R1), [AI타임즈](https://www.aitimes.com/news/articleView.html?idxno=168937)
  - 별도의 검색 단계를 거치지 않고도, 추론 과정에서 직접 검색 쿼리를 생성해 검색 엔진과 상호작용하고 검색 결과를 추론 과정에 통합할 수 있도록 설계
  - 사고 사슬(CoT)의 단계를 ▲생각(think) ▲검색(search) ▲정보(information) ▲정답(answer) 등으로 세분화
* (3/13) 알리바바, 영상 속 인물의 감정을 분석하고, 의상과 주변 환경을 설명하는 'R1-옴니(R1-Omni)' 모델 출시 -[HF](https://huggingface.co/StarJiaxing/R1-Omni-0.5B), [git](https://github.com/HumanMLLM/R1-Omni), [AI타임즈](https://www.aitimes.com/news/articleView.html?idxno=168750)
  - 시각: 시그립(SigLIP)' 모델, 오디오: '위스퍼-라지-v3' 모델, '딥시크-R1'에서 도입한 강화학습법 ‘RLVR’을 적용
* (3/11) MoE 문제 해결하는 'CoE(Chain of Expert)' 등장..."효율성과 정확성 향상" - [git](https://github.com/ZihanWang314/coe), [AI타임즈](https://www.aitimes.com/news/articleView.html?idxno=168663)
    - LLM: Dense Model -> MoE: 여러 전문가로 분할 -> CoE: 전문가를 병렬이 아닌 순차적으로 활성화(Context-Aware Input) 방식, 중간 결과 공유
* (3/10) 인셉션, 최초의 확산 방식 언어모델(dLLM) '머큐리' 공개 - [inceptionlabs](https://www.inceptionlabs.ai/blog), [Demo](https://chat.inceptionlabs.ai/), [AI타임즈](https://www.aitimes.com/news/articleView.html?idxno=168616)
   - H100 GPU에서 초당 1000개 이상의 토큰 처리가 가능, 현재 프론티어 모델 대비 5~10배 빠른 속도 측정 
 
### Feb. 2025
* (2/26) 딥시크, 저비용 고효율 'MoE 엔지니어링' 핵심 기술 'DeepEP(Deep Expert Parallelism)' 오픈 소스 공개 - [git](https://github.com/deepseek-ai/DeepEP), [AI타임즈](https://www.aitimes.com/news/articleView.html?idxno=168317)
* (2/21) MS, 로봇용 VLAM(비전-언어-행동) 통합 모델 'Magma(Multimodal Agentic Foundation)' 출시 - [arXiv](https://arxiv.org/pdf/2502.13130), [AI타임즈](https://www.aitimes.com/news/articleView.html?idxno=168198)
  - 3900만개의 샘플을 포함한 다양한 데이터셋으로 훈련
  - 비전 모델: '컨브넷Xt-XXL', 언어 모델: '라마-3-8B', UI 환경 강화: 'SoM(Set-of-Mark)'과 'ToM(Trace-of-Mark)'
* (2/21) 딥시크, 긴 컨텍스트 추론 효율 높이는 메커니즘(Native Sparse Attention) 공개 - [arXiv](https://arxiv.org/abs/2502.11089), [AI타임즈](https://www.aitimes.com/news/articleView.html?idxno=168139)
* (2/16) 딥시크, LLM 추론 성능 향상법 '코드I/O' 오픈소스 공개, "코드 대신 자연어로 추론 훈련" - [arXiv](https://arxiv.org/pdf/2502.07316), [AI타임즈](https://www.aitimes.com/news/articleView.html?idxno=168039)
