### Transformer 
* [1706.03762] Attention Is All You Need, NIPS 2017. - [arXiv](https://arxiv.org/abs/1706.03762), [review](https://towardsdatascience.com/attention-is-all-you-need-discovering-the-transformer-paper-73e5ff5e0634), [리뷰](https://misconstructed.tistory.com/62)
* http://jalammar.github.io/illustrated-transformer by Jay Alammar

### GPT-3 (OpenAI)
* GPT1 - "Improving Language Understanding by Generative Pre-Training" - [paper](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf), [리뷰](https://greeksharifa.github.io/nlp(natural%20language%20processing)%20/%20rnns/2019/08/21/OpenAI-GPT-1-Improving-Language-Understanding-by-Generative-Pre-Training/)
* GPT2 - "Language Models are Unsupervised Multitask Learners", 2019. - [paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf), [github](https://github.com/openai/gpt-2)
* GPT3 - "Language Models are Few-Shot Learners", NeurIPS 2020. - [arxiv](https://arxiv.org/abs/2005.14165), [논문리뷰](https://littlefoxdiary.tistory.com/44), [analyticsvidhya](https://www.analyticsvidhya.com/blog/2021/01/gpt-3-the-next-big-thing-foundation-of-future/)
<img src="https://www.researchgate.net/publication/344197785/figure/fig5/AS:934416989835265@1599793779247/GPT-architecture-described-in-Improving-Language-Understanding-by-Generative.ppm" width=70%>


###  A method of pre-training language representations
* Context-free models
  - word2vec or GloVe generate a single "word embedding" 
* pre-training contextual representations
  - BERT / Semi-supervised Sequence Learning, Generative Pre-Training, ELMo, and ULMFit
* ELMo
* BERT (Bidirectional Encoder Representations from Transformers)
  - a new method of pre-training language representations
  - git: https://github.com/google-research/bert
* XLNet: Generalized Autoregressive Pretraining for Language Understanding - [arxiv](https://arxiv.org/abs/1906.08237), [github](https://github.com/zihangdai/xlnet)

### Generative Models
* Megenta Project - https://magenta.tensorflow.org/welcome-to-magenta
* PixelCNN - https://github.com/igul222/pixel_rnn
* TF implementation of various GANs and VAEs.
  - https://github.com/hwalsuklee/tensorflow-generative-model-collections

### Attention 
* Attention mechanism in NLP. From seq2seq + attention to BERT - [lovit.github.io](https://lovit.github.io/machine%20learning/2019/03/17/attention_in_nlp/)

### Challenges
* TensorFlow Speech Recognition Challenge  (January 9, 2018 - Entry deadline)
  * https://www.kaggle.com/c/tensorflow-speech-recognition-challenge/

### WaveNet (DeepMind) 
* https://deepmind.com/blog/wavenet-generative-model-raw-audio/
* https://redd.it/51sr9t (@hardmaru it takes 90 minutes to synthesize one second of audio) 
* generative models for speech - https://github.com/igul222/speech
* wave2letter - End-to-End ConvNet-based SR system : http://arxiv.org/pdf/1609.03193v2.pdf


### Word Embedding 
* Word2Vec Google Code - [archive](https://code.google.com/archive/p/word2vec), [svn](http://word2vec.googlecode.com/svn/trunk)(broken), [group](https://groups.google.com/forum/#!forum/word2vec-toolkit)
* [The Illustrated Word2vec](http://jalammar.github.io/illustrated-word2vec/) by Jay Alammar
* fastText - [github](https://github.com/facebookresearch/fastText)
* StarSpace - [arxiv](https://arxiv.org/abs/1709.03856)

### Subword unit 
* Word Piece Model - [git](https://github.com/rsennrich/subword-nmt), [blog](https://lovit.github.io/nlp/2018/04/02/wpm/)
* Byte-pair Encoding (BPE) 
